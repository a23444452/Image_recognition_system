"""
è³‡æ–™é›†è™•ç†å·¥å…·
å¾ YOLO_No_Code_Training é·ç§»ä¸¦æ”¹é€ 
æ ¹æ“šæœƒè­°å…±è­˜åŠ å…¥ ProcessPoolExecutor å„ªåŒ–
"""

import yaml
import os
import shutil
import random
import logging
from pathlib import Path
from typing import Optional, Callable, List, Tuple
from concurrent.futures import ProcessPoolExecutor, as_completed

logger = logging.getLogger(__name__)


def create_data_yaml(
    train_path: str,
    val_path: str,
    class_names_str: str,
    output_path: str = "data.yaml"
) -> str:
    """
    ç”Ÿæˆ YOLO è¨“ç·´æ‰€éœ€çš„ data.yaml æª”æ¡ˆ

    Args:
        train_path: è¨“ç·´åœ–ç‰‡è·¯å¾‘
        val_path: é©—è­‰åœ–ç‰‡è·¯å¾‘
        class_names_str: é€—è™Ÿåˆ†éš”çš„é¡åˆ¥åç¨±å­—ä¸²
        output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘

    Returns:
        str: ç”Ÿæˆçš„ yaml æª”æ¡ˆçµ•å°è·¯å¾‘
    """
    # è§£æé¡åˆ¥åç¨±
    classes = [c.strip() for c in class_names_str.split(',') if c.strip()]
    names_dict = {i: name for i, name in enumerate(classes)}

    # è‹¥é©—è­‰è·¯å¾‘ç¼ºå¤±ï¼Œä½¿ç”¨è¨“ç·´è·¯å¾‘
    if not val_path:
        val_path = train_path
        logger.warning("é©—è­‰è·¯å¾‘ç¼ºå¤±ï¼Œä½¿ç”¨è¨“ç·´è·¯å¾‘")

    # å»ºç«‹é…ç½®
    data = {
        'path': os.path.abspath(os.path.dirname(output_path)),
        'train': os.path.abspath(train_path),
        'val': os.path.abspath(val_path),
        'names': names_dict,
        'nc': len(classes)
    }

    # å¯«å…¥ yaml
    with open(output_path, 'w', encoding='utf-8') as f:
        yaml.dump(data, f, sort_keys=False, allow_unicode=True)

    logger.info(f"âœ… å·²ç”Ÿæˆ data.yaml: {output_path}")
    logger.info(f"   - {len(classes)} å€‹é¡åˆ¥")

    return os.path.abspath(output_path)


def split_dataset(
    source_folder: str,
    output_folder: str,
    split_ratio: float = 0.8,
    progress_callback: Optional[Callable[[str], None]] = None,
    use_multiprocessing: bool = True,
    max_workers: int = 4
) -> Tuple[int, int]:
    """
    å°‡åŸå§‹è³‡æ–™é›†åˆ†å‰²ç‚º YOLO train/val çµæ§‹

    æ ¹æ“šæœƒè­°å…±è­˜ï¼šä½¿ç”¨ ProcessPoolExecutor åŠ é€Ÿæª”æ¡ˆè¤‡è£½

    Args:
        source_folder: åŒ…å«åœ–ç‰‡å’Œæ¨™è¨»çš„åŸå§‹è³‡æ–™å¤¾
        output_folder: ç›®æ¨™è³‡æ–™å¤¾
        split_ratio: è¨“ç·´é›†æ¯”ä¾‹ (0.0 åˆ° 1.0)
        progress_callback: é€²åº¦å›èª¿å‡½æ•¸
        use_multiprocessing: æ˜¯å¦ä½¿ç”¨å¤šé€²ç¨‹åŠ é€Ÿ
        max_workers: æœ€å¤§ worker æ•¸é‡

    Returns:
        Tuple[int, int]: (è¨“ç·´é›†åœ–ç‰‡æ•¸, é©—è­‰é›†åœ–ç‰‡æ•¸)
    """
    source = Path(source_folder)
    dest = Path(output_folder)

    if not source.exists():
        raise FileNotFoundError(f"ä¾†æºè³‡æ–™å¤¾ä¸å­˜åœ¨: {source}")

    # å»ºç«‹ç›®éŒ„çµæ§‹
    (dest / 'images' / 'train').mkdir(parents=True, exist_ok=True)
    (dest / 'images' / 'val').mkdir(parents=True, exist_ok=True)
    (dest / 'labels' / 'train').mkdir(parents=True, exist_ok=True)
    (dest / 'labels' / 'val').mkdir(parents=True, exist_ok=True)

    # å–å¾—æ‰€æœ‰åœ–ç‰‡
    valid_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}
    images = [f for f in source.iterdir() if f.suffix.lower() in valid_exts]

    if not images:
        msg = "ä¾†æºè³‡æ–™å¤¾ä¸­æœªæ‰¾åˆ°åœ–ç‰‡"
        logger.warning(msg)
        if progress_callback:
            progress_callback(msg)
        return 0, 0

    # éš¨æ©Ÿæ‰“äº‚
    random.shuffle(images)

    # åˆ†å‰²
    split_idx = int(len(images) * split_ratio)
    train_imgs = images[:split_idx]
    val_imgs = images[split_idx:]

    msg = f"ğŸ“Š æ‰¾åˆ° {len(images)} å¼µåœ–ç‰‡ã€‚åˆ†å‰²: {len(train_imgs)} è¨“ç·´, {len(val_imgs)} é©—è­‰"
    logger.info(msg)
    if progress_callback:
        progress_callback(msg)

    # è¤‡è£½æª”æ¡ˆå‡½æ•¸
    def copy_file_pair(img_path: Path, dest_folder: Path, split_type: str):
        """è¤‡è£½åœ–ç‰‡å’Œå°æ‡‰çš„æ¨™è¨»æª”"""
        try:
            # è¤‡è£½åœ–ç‰‡
            shutil.copy2(img_path, dest_folder / 'images' / split_type / img_path.name)

            # è¤‡è£½æ¨™è¨»ï¼ˆè‹¥å­˜åœ¨ï¼‰
            label_path = img_path.with_suffix('.txt')
            if label_path.exists():
                shutil.copy2(label_path, dest_folder / 'labels' / split_type / label_path.name)

            return True
        except Exception as e:
            logger.error(f"è¤‡è£½å¤±æ•— {img_path.name}: {e}")
            return False

    # è¤‡è£½æª”æ¡ˆï¼ˆä½¿ç”¨å¤šé€²ç¨‹å„ªåŒ–ï¼‰
    if use_multiprocessing and len(images) > 100:
        msg = f"âš¡ ä½¿ç”¨ {max_workers} å€‹ worker åŠ é€Ÿè¤‡è£½..."
        logger.info(msg)
        if progress_callback:
            progress_callback(msg)

        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # è¨“ç·´é›†
            train_futures = [
                executor.submit(copy_file_pair, img, dest, 'train')
                for img in train_imgs
            ]
            # é©—è­‰é›†
            val_futures = [
                executor.submit(copy_file_pair, img, dest, 'val')
                for img in val_imgs
            ]

            # ç­‰å¾…å®Œæˆ
            for future in as_completed(train_futures + val_futures):
                future.result()
    else:
        # å–®é€²ç¨‹è¤‡è£½ï¼ˆå°å‹è³‡æ–™é›†ï¼‰
        for img in train_imgs:
            copy_file_pair(img, dest, 'train')
        for img in val_imgs:
            copy_file_pair(img, dest, 'val')

    # è™•ç† classes.txt
    classes_file = source / 'classes.txt'
    if classes_file.exists():
        shutil.copy2(classes_file, dest / 'classes.txt')
        msg = "âœ… å·²è¤‡è£½ classes.txt"
        logger.info(msg)
        if progress_callback:
            progress_callback(msg)

    msg = f"âœ… è³‡æ–™é›†æº–å‚™å®Œæˆ: {dest}"
    logger.info(msg)
    if progress_callback:
        progress_callback(msg)

    return len(train_imgs), len(val_imgs)


def validate_dataset(dataset_folder: str) -> dict:
    """
    é©—è­‰è³‡æ–™é›†çµæ§‹èˆ‡å®Œæ•´æ€§

    Args:
        dataset_folder: è³‡æ–™é›†è³‡æ–™å¤¾è·¯å¾‘

    Returns:
        dict: é©—è­‰çµæœ
    """
    folder = Path(dataset_folder)
    result = {
        'valid': True,
        'errors': [],
        'warnings': [],
        'stats': {}
    }

    # æª¢æŸ¥ç›®éŒ„çµæ§‹
    required_dirs = [
        'images/train',
        'images/val',
        'labels/train',
        'labels/val'
    ]

    for dir_path in required_dirs:
        if not (folder / dir_path).exists():
            result['valid'] = False
            result['errors'].append(f"ç¼ºå°‘ç›®éŒ„: {dir_path}")

    if not result['valid']:
        return result

    # çµ±è¨ˆåœ–ç‰‡èˆ‡æ¨™è¨»
    for split in ['train', 'val']:
        img_dir = folder / 'images' / split
        label_dir = folder / 'labels' / split

        images = list(img_dir.glob('*.*'))
        labels = list(label_dir.glob('*.txt'))

        img_count = len(images)
        label_count = len(labels)

        result['stats'][split] = {
            'images': img_count,
            'labels': label_count
        }

        # è­¦å‘Šï¼šæ¨™è¨»æ•¸é‡ä¸åŒ¹é…
        if img_count != label_count:
            result['warnings'].append(
                f"{split} é›†åœ–ç‰‡æ•¸ ({img_count}) èˆ‡æ¨™è¨»æ•¸ ({label_count}) ä¸åŒ¹é…"
            )

    logger.info(f"è³‡æ–™é›†é©—è­‰å®Œæˆ: {result}")
    return result


def get_dataset_statistics(dataset_folder: str) -> dict:
    """
    å–å¾—è³‡æ–™é›†çµ±è¨ˆè³‡è¨Š

    Args:
        dataset_folder: è³‡æ–™é›†è³‡æ–™å¤¾è·¯å¾‘

    Returns:
        dict: çµ±è¨ˆè³‡è¨Š
    """
    folder = Path(dataset_folder)
    stats = {
        'total_images': 0,
        'total_labels': 0,
        'train': {},
        'val': {},
        'class_distribution': {}
    }

    for split in ['train', 'val']:
        img_dir = folder / 'images' / split
        label_dir = folder / 'labels' / split

        if img_dir.exists():
            images = list(img_dir.glob('*.*'))
            stats[split]['images'] = len(images)
            stats['total_images'] += len(images)

        if label_dir.exists():
            labels = list(label_dir.glob('*.txt'))
            stats[split]['labels'] = len(labels)
            stats['total_labels'] += len(labels)

            # çµ±è¨ˆé¡åˆ¥åˆ†å¸ƒ
            for label_file in labels:
                try:
                    with open(label_file, 'r') as f:
                        for line in f:
                            cls_id = int(line.split()[0])
                            stats['class_distribution'][cls_id] = \
                                stats['class_distribution'].get(cls_id, 0) + 1
                except Exception as e:
                    logger.warning(f"è®€å–æ¨™è¨»å¤±æ•— {label_file}: {e}")

    return stats
